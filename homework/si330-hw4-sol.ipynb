{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 330: Homework 4: APIs on AWS\n",
    "\n",
    "\n",
    "## Due: Friday, February 9, 2018,  11:59:00pm\n",
    "\n",
    "### Submission instructions</font>\n",
    "After completing this homework, you will turn in two files via Canvas ->  Assignments -> HW 4:\n",
    "Your Notebook, named si330-hw4-YOUR_UNIQUE_NAME.ipynb and\n",
    "the HTML file, named si330-hw4-YOUR_UNIQUE_NAME.html.\n",
    "\n",
    "### Name:  YOUR NAME GOES HERE\n",
    "### Uniqname: YOUR UNIQNAME GOES HERE\n",
    "### People you worked with: [if you didn't work with anyone else write \"I worked by myself\" here].\n",
    "\n",
    "## Top-Level Goal\n",
    "To create a microservice that returns the counts of all bigrams in a text passage.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "After completing this Lab, you should know how to:\n",
    "* create an AWS Lambda function that takes a string and returns the counts of all bigrams in that text\n",
    "* write an AWS API Gateway integration which allows both GET and POST requests to access an AWS Lambda\n",
    "* write documenation to the microservice that you've created\n",
    "\n",
    "### Note: See end of notebook for notes about going \"Above and Beyond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Steps For Analysis\n",
    "Here's an overview of the steps that you'll need to do to complete this lab.\n",
    "2. Upload data to an S3 bucket\n",
    "1. Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text, both via a POST request with the text and via a GET request to a URL that returns the text (e.g. an S3 bucket)\n",
    "3. Create a python code block in this notebook to demonstrate the functionality of your microservice\n",
    "\n",
    "Each of these steps is detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload data to an S3 bucket\n",
    "To get ready to test the POST functionality of the code you generate in the next step, you should upload a text file that is **500 or fewer lines** to an S3 bucket.  See the description of CORS for an explanation of why we want to put the data in the same domain (amazonaws.com) as the Lambda.\n",
    "\n",
    "Follow the same approach that we used in the lab to upload a small text file to your S3 bucket, ensuring that the permissions are set to allow public access\n",
    "\n",
    "### <font color=\"magenta\">Q1: Enter the URL of your text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Rubric\n",
    "\n",
    "5 points for putting an url here (Do not deduct points, if they are getting mostly full otherwise - or where it seem might an oversight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your S3 text file's URL here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text\n",
    "\n",
    "Similar to what we did in the lab, you're going to create a microservice that consists of two parts: an AWS Lambda and an API Gateway.  You can use exactly the same technique that we did in the lab to get started.\n",
    "\n",
    "You will need to modify the code in the Lambda to handle two types of requests:\n",
    "1. A GET request with a queryStringParameter of url=http://some.url.goes.here/text.txt, which specifies the location of the text to be processed and\n",
    "2. A POST request with the text to be processed included as the \"text\" value in the body payload.\n",
    "\n",
    "### The following code block is a reasonable starting point for creating your Lambda.  Note that this code should not be run in this notebook but rather serve as the starting point for your work in the Lambda editor.\n",
    "\n",
    "**NOTE** Please see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python for hints about how to create bigrams without NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Rubric\n",
    "\n",
    "30 points\n",
    "* If the lambda function is handling both GET and POST correctly.\n",
    "* The URL is being dynamically retrieved for GET **(10 pts)**\n",
    "* The text data is correctly being retrieved in the GET method **(10 pts)**\n",
    "* The text is normalized **(2 points)**\n",
    "* The text is tokenized (this can be tokenized by words or characters - but they should provide an explanation; Some people may have just copied the code from StackOverflow which would return bigrams of characters) **(5 points)**\n",
    "* Counts are correct (check using some other text file) **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PUT SOME DOCUMENTATION HERE\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_bigrams(text):\n",
    "    sent_tokens = re.split(r'[.?!]', text.lower())\n",
    "    bigrams = [b for l in sent_tokens for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "    bigram_counter = defaultdict(int)\n",
    "    for bigram in bigrams:\n",
    "        bigram_counter[str(bigram)] += 1\n",
    "    return bigram_counter\n",
    "    \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"text\": \"\"}\n",
    "    # Handle GET method\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            url = params['url']\n",
    "            text = requests.get(url).text\n",
    "            d['text'] = get_bigrams(text)\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            d['text'] = get_bigrams(body['text'])\n",
    "            \n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2a: Enter the URL of your Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your Lambda's URL here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2b: Copy your final Lambda code into the following code block (but do not run it here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your lambda code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate the GET and POST functionality of your Lambda\n",
    "\n",
    "### <font color=\"magenta\">Q3: Create a code block that uses `requests` to demonstrate the functionality of your Lambda.  You can modify the template below or create your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Rubric\n",
    "\n",
    "5 points\n",
    "* There's not much to do here. If they put in some effort to print out properly - give them 5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it', 'was') 10\n",
      "('was', 'the') 10\n",
      "('the', 'best') 1\n",
      "('best', 'of') 1\n",
      "('of', 'times,') 2\n",
      "('the', 'worst') 1\n",
      "('worst', 'of') 1\n",
      "('the', 'age') 2\n",
      "('age', 'of') 2\n",
      "('of', 'wisdom,') 1\n",
      "('of', 'foolishness,') 1\n",
      "('the', 'epoch') 2\n",
      "('epoch', 'of') 2\n",
      "('of', 'belief,') 1\n",
      "('of', 'incredulity,') 1\n",
      "('the', 'season') 2\n",
      "('season', 'of') 2\n",
      "('of', 'light,') 1\n",
      "('of', 'darkness,') 1\n",
      "('the', 'spring') 1\n",
      "('spring', 'of') 1\n",
      "('of', 'hope,') 1\n",
      "('the', 'winter') 1\n",
      "('winter', 'of') 1\n",
      "('of', 'despair,') 1\n",
      "('we', 'had') 2\n",
      "('had', 'everything') 1\n",
      "('everything', 'before') 1\n",
      "('before', 'us,') 2\n",
      "('had', 'nothing') 1\n",
      "('nothing', 'before') 1\n",
      "('we', 'were') 2\n",
      "('were', 'all') 2\n",
      "('all', 'going') 2\n",
      "('going', 'direct') 2\n",
      "('direct', 'to') 1\n",
      "('to', 'heaven,') 1\n",
      "('direct', 'the') 1\n",
      "('the', 'other') 1\n",
      "('other', 'way--') 1\n",
      "('in', 'short,') 1\n",
      "('short,', 'the') 1\n",
      "('the', 'period') 1\n",
      "('period', 'was') 1\n",
      "('was', 'so') 1\n",
      "('so', 'far') 1\n",
      "('far', 'like') 1\n",
      "('like', 'the') 1\n",
      "('the', 'present') 1\n",
      "('present', 'period,') 1\n",
      "('period,', 'that') 1\n",
      "('that', 'some') 1\n",
      "('some', 'of') 1\n",
      "('its', 'noisiest') 1\n",
      "('noisiest', 'authorities') 1\n",
      "('authorities', 'insisted') 1\n",
      "('insisted', 'on') 1\n",
      "('on', 'its') 1\n",
      "('its', 'being') 1\n",
      "('being', 'received,') 1\n",
      "('received,', 'for') 1\n",
      "('for', 'good') 1\n",
      "('good', 'or') 1\n",
      "('or', 'for') 1\n",
      "('evil,', 'in') 1\n",
      "('in', 'the') 1\n",
      "('the', 'superlative') 1\n",
      "('superlative', 'degree') 1\n",
      "('degree', 'of') 1\n",
      "('of', 'comparison') 1\n",
      "('comparison', 'only') 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://fovsbrjx2i.execute-api.us-east-2.amazonaws.com/prod/bigrammer' # change this URL\n",
    "textURL = 'https://s3.us-east-2.amazonaws.com/abhsarma-week4/dickens-totc.txt' # change this URL\n",
    "\n",
    "# Demonstrate the GET functionality by passing the URL of your text file in S3 to your Lambda as a GET request\n",
    "response = requests.get(lambdaURL + '?url=' + textURL)\n",
    "bigrams = json.loads(response.text)[\"text\"]\n",
    "for k, v in bigrams.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it', 'was') 10\n",
      "('was', 'the') 10\n",
      "('the', 'best') 1\n",
      "('best', 'of') 1\n",
      "('of', 'times,') 2\n",
      "('the', 'worst') 1\n",
      "('worst', 'of') 1\n",
      "('the', 'age') 2\n",
      "('age', 'of') 2\n",
      "('of', 'wisdom,') 1\n",
      "('of', 'foolishness,') 1\n",
      "('the', 'epoch') 2\n",
      "('epoch', 'of') 2\n",
      "('of', 'belief,') 1\n",
      "('of', 'incredulity,') 1\n",
      "('the', 'season') 2\n",
      "('season', 'of') 2\n",
      "('of', 'light,') 1\n",
      "('of', 'darkness,') 1\n",
      "('the', 'spring') 1\n",
      "('spring', 'of') 1\n",
      "('of', 'hope,') 1\n",
      "('the', 'winter') 1\n",
      "('winter', 'of') 1\n",
      "('of', 'despair,') 1\n",
      "('we', 'had') 2\n",
      "('had', 'everything') 1\n",
      "('everything', 'before') 1\n",
      "('before', 'us,') 2\n",
      "('had', 'nothing') 1\n",
      "('nothing', 'before') 1\n",
      "('we', 'were') 2\n",
      "('were', 'all') 2\n",
      "('all', 'going') 2\n",
      "('going', 'direct') 2\n",
      "('direct', 'to') 1\n",
      "('to', 'heaven,') 1\n",
      "('direct', 'the') 1\n",
      "('the', 'other') 1\n",
      "('other', 'way--') 1\n",
      "('in', 'short,') 1\n",
      "('short,', 'the') 1\n",
      "('the', 'period') 1\n",
      "('period', 'was') 1\n",
      "('was', 'so') 1\n",
      "('so', 'far') 1\n",
      "('far', 'like') 1\n",
      "('like', 'the') 1\n",
      "('the', 'present') 1\n",
      "('present', 'period,') 1\n",
      "('period,', 'that') 1\n",
      "('that', 'some') 1\n",
      "('some', 'of') 1\n",
      "('its', 'noisiest') 1\n",
      "('noisiest', 'authorities') 1\n",
      "('authorities', 'insisted') 1\n",
      "('insisted', 'on') 1\n",
      "('on', 'its') 1\n",
      "('its', 'being') 1\n",
      "('being', 'received,') 1\n",
      "('received,', 'for') 1\n",
      "('for', 'good') 1\n",
      "('good', 'or') 1\n",
      "('or', 'for') 1\n",
      "('evil,', 'in') 1\n",
      "('in', 'the') 1\n",
      "('the', 'superlative') 1\n",
      "('superlative', 'degree') 1\n",
      "('degree', 'of') 1\n",
      "('of', 'comparison') 1\n",
      "('comparison', 'only') 1\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the POST functionality by passing the text as a JSON parameter to requests.post()\n",
    "# note that we retrieve the contents of the S3 bucket using requests.get()\n",
    "s3text = requests.get(textURL).text # get the text from the bucket\n",
    "d = {\"text\": s3text}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "bigrams = json.loads(response.text)['text']\n",
    "for k, v in bigrams.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your notebook, download it as HTML and submit both the .ipynb and .html files to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about going \"Above and Beyond\"\n",
    "\n",
    "There are ample opportunities for extending this homework assignment.  You might, for example, decide to break the microservice into three separate ones (normalizing, tokenizing, and creating bigrams).  Alternatively, you might invest time into getting NLTK data into Lambda so you can use its functionality (see https://stackoverflow.com/questions/42394335/paths-in-aws-lambda-with-python-nltk).  Another interesting investigation might be to use the addition of a data file to an S3 bucket as a trigger to run the bigram analysis, perhaps writing the results to another (public) bucket.\n",
    "\n",
    "**IF YOU CHOOSE TO GO ABOVE AND BEYOND, YOU _MUST_ CHANGE THE FOLLOWING MARKDOWN BLOCK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above and Beyond\n",
    "\n",
    "Indicate here why you believe that your work should be considered \"above and beyond\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
