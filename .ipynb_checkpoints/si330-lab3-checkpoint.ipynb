{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Natural Language Processing with NLTK\n",
    "\n",
    "\n",
    "## Due: Thursday, January 25, 2018,  11:59:00pm\n",
    "\n",
    "### Submission instructions\n",
    "After completing this homework, you will turn in two files via Canvas ->  Assignments -> Lab 3:\n",
    "Your Notebook, named si330-lab3-YOUR_UNIQUE_NAME.ipynb and\n",
    "the HTML file, named si330-lab3-YOUR_UNIQUE_NAME.html\n",
    "\n",
    "### Name:  YOUR NAME GOES HERE\n",
    "### Uniqname: YOUR UNIQNAME GOES HERE\n",
    "### People you worked with: [if you didn't work with anyone else write \"I worked by myself\" here].\n",
    "\n",
    "\n",
    "## Objectives\n",
    "After completing this Lab, you should know how to use NLTK to:\n",
    "* Normalize and Tokenize your text data\n",
    "* Parts of Speech tagging of a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing NLTK\n",
    "\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python.\n",
    "\n",
    "You will install a package directly from Jupyter Notebooks.\n",
    "\n",
    "<b>Make sure you are in the SI 330 environment when you run your Jupyter notebook. In your Jupyter notebook run the following command</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: http://nltk.org/nltk_data/\n",
    "\n",
    "In the next code chunk, you will install the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can remove this cell once you've installed the corpora\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "NLTK's corpora contains texts from the Gutenberg project. In today's lab we will be working on text from Shakespeare's Julius Caesar. In the chunk below, you can see what books are available in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Texts present in the Gutenberg Corpora\n",
    "for i in nltk.corpus.gutenberg.fileids():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import Julius Caesar and save it in a variable.\n",
    "### <font color=\"magenta\">Print the first 1000 characters to see what the text looks like.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to get Julius Caesar as raw text. \n",
    "# There are other ways in which you could load text from this corpus, but we will use raw text\n",
    "caesar = nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')\n",
    "\n",
    "# Print the first 1000 characters of Julius Caesar. Why are we printing characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Tokenization\n",
    "\n",
    "Next, we will normalize and tokenize the text from the play. We will use the <b>```RegexpTokenizer```</b> from  <b>```nltk.tokenizer```</b> package. This will allow us to write our own regular expression and tokenize the text. You only want the words, so write your regular expression accordingly.\n",
    "\n",
    "### <font color=\"magenta\">Write code to normalize the text, then tokenize the text into words using regex.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# normalize the text by converting it to lower case\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'') # Fill in with the right regular expression.\n",
    "\n",
    "word_tokens = tokenizer.tokenize(None) # Pass the normalized text to this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types, tokens and type-token ration\n",
    "A useful measure to calculate is the type-token ratio (TTR). For that, we would need to calculate the total number of word types, which is the collection of unique words, and tokens, which here is an instance of a word.\n",
    "### <font color=\"magenta\">Calculate total number of word types, word tokens, and type-token ratio for the text.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to calculate the number of types, tokens and then\n",
    "# divide the number of types by the number of tokens.  \n",
    "# Note that there are multiple ways to do this\n",
    "\n",
    "type_token_ratio = None # Calculate the type-token ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "Bigrams are sequences of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n",
    "\n",
    "Here, you will retrieve all the bigrams from the text, store them in a dictionary, and count the number of times each bigram occurs.  nltk makes this easy by providing a bigram() function.  You can get a list of bigrams with the following code:\n",
    "\n",
    "```\n",
    ">>> list_of_bigrams = list(nltk.bigrams(['more', 'is', 'said', 'than', 'done']))\n",
    ">>> print(list_of_bigrams)\n",
    "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]\n",
    "```\n",
    "\n",
    "The list comprehension allows us to print the results.  If you omit list() from the above statement, you get a generator, which is useful if you want to iterate over the list of bigrams (which is what you want to do below), but not so great if you want to print out the results.\n",
    "\n",
    "### <font color=\"magenta\">Calculate the bigram counts - two words occuring one after the other - and store it in a dictionary, along with the number of times it has occured.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a bigram generator using the bigrams() function and iterate over your bigrams \n",
    "# to store the count of each bigram in a dictionary.\n",
    "# Your key should be the bigrams (a 2-tuple). Your values would be the number of occurences of the bigram.\n",
    "bigram_counts = None\n",
    "\n",
    "# Implement a sorted function which returns the most commonly used bigrams in descending order\n",
    "sorted_bigram_counts = sorted()\n",
    "\n",
    "# Print out the 20 most commonly used bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "Next, we will use Part of Speech tagging to tag the words from the play. We need to pass tokenized words to this <b>```pos_tag```</b> function, which returns a list of token - tag tuples. The first two lines in the cell below import the necessary functions. \n",
    "\n",
    "### <font color=\"magenta\">Pass the word tokens into pos_tag.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use these libraries for Part of Speech tagging\n",
    "from nltk.tag import pos_tag\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_tags = pos_tag(None) # change 'None' to your list of word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the frequency distribution of each word type. We will use the <b>```nltk.FreqDist()```</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(tag for (word, tag) in word_tags)\n",
    "fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization vs. regex\n",
    "\n",
    "Get the names of all the characters (cast members for clarity) from the play. Cast members are the ones with the lines. This can be done using either <b>```nltk's RegexpTokenizer```</b> or <b>```re.findall```</b>. First try out the <b>```nltk's RegexpTokenizer```</b>. Print the set of character names. (Make sure the character names don't appear twice.)  If you have time and energy, write code to do the same thing using re.findall().\n",
    "\n",
    "<b>Note: we will be using the raw text, stored in the variable ```caesar``` for this, and not your tokenized words.</b>\n",
    "\n",
    "Consider the following excerpt:\n",
    "```\n",
    "  Flauius. Hence: home you idle Creatures, get you home:\n",
    "Is this a Holiday? What, know you not\n",
    "(Being Mechanicall) you ought not walke\n",
    "Vpon a labouring day, without the signe\n",
    "Of your Profession? Speake, what Trade art thou?\n",
    "  Car. Why Sir, a Carpenter\n",
    "```\n",
    "\n",
    "There are two cast members in the above text: Flauius and Car.  Don't worry about the fact that Car is an abbreviation.  You should notice that the cast member names are preceded by a variable number of spaces at \n",
    "the beginning of a line, followed by a single word, followed by a period.\n",
    "\n",
    "The tokenizer is a method which you will use and store the output in word_tokens. \n",
    "\n",
    "### <font color=\"magenta\">Write the regular expression to tokenize and return only the cast member names and print out the names of the cast members and the number of different cast members.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'') #Fill in with the right regular expression.\n",
    "\n",
    "# You will need to make some changes to this function\n",
    "cast_member_tokens = tokenizer.tokenize(None) # This should give you the tokens\n",
    "print(...) # You will need to print the types (not the tokens) to get the unique cast member names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
