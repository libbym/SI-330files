{"cells":[{"cell_type":"markdown","source":["# SI 330 WN 2018 - Lab 10: Using the Spark Dataframes\n\n## Objectives\n1. Gaining familiarity with Spark Dataframes\n2. Get experience with reading schemas of the data\n3. Learning basic operations like column selection, sorting, grouping\n4. Familiarity with the Yelp dataset\n\n## Please fill in...\n* Your name: Manali \n* People you worked with:  [If you didn't work with anyone, write \"I worked by myself\" here]\n\n## Submission Instructions:\nPlease turn in this Databricks notebook file .html formats as well as the URL to your published notebook.\n\n## Overview"],"metadata":{}},{"cell_type":"code","source":["import re\nimport pyspark\nfrom pyspark import SparkContext"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["ACCESS_KEY = \"AKIAIPKMRL4G3IEVQ7FQ\"\nSECRET_KEY = \"bkG5SUmSc+S8bQseSo8SaBAHQtt3xGUfRlOojUrW\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"umsi-data-science-west\"\nMOUNT_NAME = \"umsi-data-science\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\ndisplay(dbutils.fs.ls(\"/mnt/umsi-data-science/si330wn2018\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Step 0: Load the dataset\n\nThe Yelp business dataset is available as both JSON and SQL. We've provided the address to the JSON file in an S3 bucket.\n\n#### Load the JSON file into a dataframe"],"metadata":{}},{"cell_type":"code","source":["# load the JSON file\ndf = spark.read.json('dbfs:/mnt/umsi-data-science/si330wn2018/business.json')\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Prints out the first two lines to see what it looks like.\ndf.take(2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Looking at the schema of the dataframe\n\nThere are a lot of columns in the dataset. It might be helpful to look at the schema."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Step 1: Filtering\n\n** 1.1 We first give you an example of how you'd filter a dataframe. The chunk below filters all listings which has a rating less than 4.**\n\n** 1.2 We've provided a list of states in `us_states_list`. Filter the dataframe so that rows from only these cities remain and store it in a new dataframe named `df_filtered_states`.**"],"metadata":{}},{"cell_type":"code","source":["# this filters the dataframe and prints it out. But it doesn't store the filtered df\ndf.filter(df['stars'] >= 4).show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["us_states_list = [\"AZ\", \"NV\", \"OH\", \"PA\"]\n\n# Write your code here to store only the places from these four states\ndf_filtered_states = df.filter(df['state'].isin(us_states_list))\ndf_filtered_states.take(4)\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#### Step 2: Grouping and Sorting\n\nWe can perform similar grouping and sorting operations on the dataframe like we did in Pandas.\n\n** 2.1 Count the frequency of the different stars for the data in the dataframe `df_filtered_states` (Hint: look at the slides) **"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\ndf_filtered_states.groupby('stars').count().show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["** 2.2 Count the number of listings in each states and sort them in descending order **"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\ndf_filtered_states.groupby('state').count().sort('state', ascending = False).show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### Step 3: Joining\n\n* **3.1: Load the json file `tip.json` as a dataframe named `df_tip`**\n* **3.2: Take a look at what this dataframe looks like**\n* **3.3: Join the `df_filtered_states` dataframe with `df_tip` dataframe** \n* **3.4: Find the listings with the most tips**"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\ndf_tip = spark.read.json('dbfs:/mnt/umsi-data-science/si330wn2018/tip.json')\n# df_tip.take(8)\n# df_tip.printSchema()\n\ndf_joined = df_filtered_states.join(df_tip, df_filtered_states.business_id == df_tip.business_id)\n\n# df_joined.groupby(df_tip.business_id).count().sort('business_id', ascending = True).show()\ndf_joined.take(5)\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Step 4: Setting up SQL\n\n* **4.1: Create a temporary sql table from your dataframe**"],"metadata":{}},{"cell_type":"code","source":["# Write your code here\n\ndf_filtered_states.createOrReplaceTempView('yelp')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["* **4.2 Write a SQL query to get the names, address, stars and review counts of intimate and hipster places in Phoenix which are rated 3 stars or more from the SQL table you created in the previous step**"],"metadata":{}},{"cell_type":"code","source":["query = \"\"\"\nselect * from yelp\nwhere\ncity == 'Pheonix'\nand\nstars >= 3.0\nand\nattributes.Ambience.hipster\nand\nattributes.Ambience.intimate\n\"\"\"\n\nsqlDF = sql(query)\nsqlDF.show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"si330wn18-lab10","notebookId":759695569037893},"nbformat":4,"nbformat_minor":0}
